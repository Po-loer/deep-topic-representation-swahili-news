{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQYXcSxiq4_T"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install datasets transformers sentence-transformers umap-learn plotly\n",
        "!pip install gensim nltk seaborn matplotlib pandas numpy scikit-learn\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install tensorflow\n",
        "!pip install googletrans==4.0.0-rc1  # For machine translation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"All libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the Swahili News dataset\n",
        "print(\"Loading Swahili News dataset...\")\n",
        "dataset = load_dataset(\"mteb/swahili_news\")\n",
        "\n",
        "# Let's explore the dataset structure\n",
        "print(\"\\nDataset structure:\")\n",
        "print(dataset)\n",
        "\n",
        "# Check the first few samples\n",
        "print(\"\\nFirst sample from training set:\")\n",
        "print(dataset['train'][0])\n",
        "\n",
        "# Check dataset features/columns\n",
        "print(\"\\nDataset features:\")\n",
        "print(dataset['train'].features)"
      ],
      "metadata": {
        "id": "3BZ0ouCcrfyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Convert to pandas DataFrame for easier manipulation\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "df_test = pd.DataFrame(dataset['test'])\n",
        "\n",
        "print(\"Training set shape:\", df_train.shape)\n",
        "print(\"Test set shape:\", df_test.shape)\n",
        "\n",
        "print(\"\\nFirst few rows of training data:\")\n",
        "print(df_train.head())\n",
        "\n",
        "print(\"\\nColumn names:\")\n",
        "print(df_train.columns.tolist())"
      ],
      "metadata": {
        "id": "tcNRVwQNr1Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 18.1: Text Length Distribution\n",
        "print(\"=== EXPLORATORY DATA ANALYSIS ===\")\n",
        "\n",
        "# Calculate text lengths\n",
        "df_train['text_length'] = df_train['text'].apply(len)\n",
        "df_train['word_count'] = df_train['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Plot text length distribution\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df_train['text_length'], bins=50, alpha=0.7, color='skyblue')\n",
        "plt.xlabel('Text Length (characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Text Length (Characters)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df_train['word_count'], bins=50, alpha=0.7, color='lightcoral')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Word Count')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Text Length Statistics:\")\n",
        "print(f\"Average characters: {df_train['text_length'].mean():.2f}\")\n",
        "print(f\"Max characters: {df_train['text_length'].max()}\")\n",
        "print(f\"Min characters: {df_train['text_length'].min()}\")\n",
        "print(f\"Average word count: {df_train['word_count'].mean():.2f}\")"
      ],
      "metadata": {
        "id": "Q7WS0p7jsj8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK resources\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"NLTK resources downloaded successfully!\")\n",
        "\n",
        "# Now continue with the word frequency analysis\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Custom Swahili stopwords since NLTK doesn't have them\n",
        "swahili_stopwords = {\n",
        "    'na', 'ya', 'wa', 'kwa', 'za', 'katika', 'ni', 'la', 'kuwa', 'kama',\n",
        "    'hiyo', 'yake', 'zao', 'hii', 'wake', 'kwenye', 'haya', 'hao', 'hapa',\n",
        "    'hapo', 'hadi', 'hivyo', 'huo', 'ile', 'hili', 'huu', 'wetu', 'yetu',\n",
        "    'sana', 'pia', 'tu', 'kati', 'mwa', 'kuhusu', 'baada', 'kabla', 'kwanza',\n",
        "    'bila', 'hata', 'lakini', 'au', 'ama', 'ila', 'ingawa', 'ambao', 'ambaye'\n",
        "}\n",
        "\n",
        "def preprocess_swahili_text(text):\n",
        "    \"\"\"Preprocess Swahili text for frequency analysis\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and short words\n",
        "    tokens = [word for word in tokens if word not in swahili_stopwords and len(word) > 2]\n",
        "    return tokens\n",
        "\n",
        "# Process all texts and count word frequencies\n",
        "print(\"Processing texts for word frequency analysis...\")\n",
        "all_tokens = []\n",
        "for text in df_train['text'][:1000]:  # Process first 1000 for speed\n",
        "    tokens = preprocess_swahili_text(text)\n",
        "    all_tokens.extend(tokens)\n",
        "\n",
        "# Get most common words\n",
        "word_freq = Counter(all_tokens)\n",
        "most_common_words = word_freq.most_common(20)\n",
        "\n",
        "print(\"\\nTop 20 Most Common Words:\")\n",
        "for word, freq in most_common_words:\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "# Plot word frequency\n",
        "plt.figure(figsize=(12, 6))\n",
        "words, frequencies = zip(*most_common_words)\n",
        "plt.bar(words, frequencies, color='lightseagreen')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Common Words in Swahili News')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jncKF8D7t3Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 19 (Revised): Preprocess text with Machine Translation and create embeddings\n",
        "from googletrans import Translator\n",
        "\n",
        "print(\"=== STEP 19: Text Preprocessing with Machine Translation ===\")\n",
        "\n",
        "# Initialize translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_swahili_to_english(text, max_chars=5000):\n",
        "    \"\"\"Translate Swahili text to English for understanding\"\"\"\n",
        "    try:\n",
        "        # Take first max_chars characters to avoid API limits\n",
        "        text_sample = text[:max_chars]\n",
        "        translation = translator.translate(text_sample, src='sw', dest='en')\n",
        "        return translation.text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        return \"Translation unavailable\"\n",
        "\n",
        "# Test translation on a few samples\n",
        "print(\"Testing machine translation on sample articles...\")\n",
        "for i in range(3):\n",
        "    swahili_text = df_train['text'].iloc[i][:500]  # First 500 chars\n",
        "    english_translation = translate_swahili_to_english(swahili_text)\n",
        "\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    print(f\"SWAHILI: {swahili_text}...\")\n",
        "    print(f\"ENGLISH: {english_translation}...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Translate the most common words we found earlier\n",
        "print(\"\\nTranslating most common Swahili words to English:\")\n",
        "common_words_translated = []\n",
        "for word, freq in most_common_words[:10]:\n",
        "    try:\n",
        "        translation = translator.translate(word, src='sw', dest='en')\n",
        "        common_words_translated.append((word, translation.text, freq))\n",
        "        print(f\"'{word}' -> '{translation.text}' (frequency: {freq})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not translate '{word}': {e}\")"
      ],
      "metadata": {
        "id": "iDGrRdosvy-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 20 (Full Dataset): Modeling - Train a Deep Autoencoder with ALL articles\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"=== STEP 20: Deep Autoencoder Training with FULL DATASET ===\")\n",
        "\n",
        "# Use ALL training articles (17,789 articles)\n",
        "full_texts = df_train['text'].apply(preprocess_text).tolist()\n",
        "full_labels = df_train['label_name'].tolist()\n",
        "\n",
        "print(f\"Using ALL {len(full_texts)} articles for autoencoder training\")\n",
        "\n",
        "# Generate embeddings for the full dataset\n",
        "print(\"Generating embeddings for full dataset...\")\n",
        "sentence_embeddings = model.encode(full_texts, show_progress_bar=True)\n",
        "print(f\"Embedding shape: {sentence_embeddings.shape}\")\n",
        "\n",
        "# Split data for training and validation\n",
        "X_train, X_val = train_test_split(sentence_embeddings, test_size=0.2, random_state=42)\n",
        "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n",
        "\n",
        "# Define Autoencoder architecture\n",
        "input_dim = sentence_embeddings.shape[1]  # 384 dimensions\n",
        "encoding_dim = 2  # Compress to 2D latent space\n",
        "\n",
        "# Encoder\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoded = Dense(128, activation='relu')(input_layer)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "encoded = Dense(32, activation='relu')(encoded)\n",
        "encoded = Dense(16, activation='relu')(encoded)\n",
        "latent = Dense(encoding_dim, activation='relu')(encoded)  # 2D latent space\n",
        "\n",
        "# Decoder\n",
        "decoded = Dense(16, activation='relu')(latent)\n",
        "decoded = Dense(32, activation='relu')(decoded)\n",
        "decoded = Dense(64, activation='relu')(decoded)\n",
        "decoded = Dense(128, activation='relu')(decoded)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "# Autoencoder model\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "print(\"\\nAutoencoder Architecture:\")\n",
        "autoencoder.summary()\n",
        "\n",
        "# Train the autoencoder\n",
        "print(\"\\nTraining Autoencoder on FULL dataset...\")\n",
        "history = autoencoder.fit(\n",
        "    X_train, X_train,\n",
        "    epochs=50,\n",
        "    batch_size=64,  # Increased batch size for larger dataset\n",
        "    validation_data=(X_val, X_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Autoencoder training completed with FULL dataset!\")"
      ],
      "metadata": {
        "id": "wOoP-xCby5B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 20 (Continued): Analyze reconstruction loss and feature compression\n",
        "print(\"=== STEP 20 (Continued): Analyzing Reconstruction Loss and Feature Compression ===\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title('Autoencoder Training History')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Calculate reconstruction error for sample articles\n",
        "sample_reconstructions = autoencoder.predict(sentence_embeddings[:1000])\n",
        "reconstruction_errors = np.mean((sentence_embeddings[:1000] - sample_reconstructions) ** 2, axis=1)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(reconstruction_errors, bins=50, alpha=0.7, color='coral')\n",
        "plt.xlabel('Reconstruction Error (MSE)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Reconstruction Errors')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final Training Loss: {history.history['loss'][-1]:.6f}\")\n",
        "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.6f}\")\n",
        "print(f\"Average Reconstruction Error: {np.mean(reconstruction_errors):.6f}\")\n",
        "print(f\"Compression Ratio: 384D ‚Üí 2D (192:1 compression)\")\n",
        "\n",
        "# Extract the encoder part for latent space representation\n",
        "encoder = Model(input_layer, latent)\n",
        "latent_representations = encoder.predict(sentence_embeddings)\n",
        "\n",
        "print(f\"\\nLatent space representations shape: {latent_representations.shape}\")\n",
        "print(f\"Sample latent vectors (first 5):\")\n",
        "for i in range(5):\n",
        "    print(f\"  Article {i+1}: [{latent_representations[i][0]:.4f}, {latent_representations[i][1]:.4f}]\")"
      ],
      "metadata": {
        "id": "hFeLUAdfzw4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 21 (Revised): Visualize Latent Clusters with English Translations\n",
        "import umap\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "print(\"=== STEP 21: Visualize Latent Clusters with English Translations ===\")\n",
        "\n",
        "# Translate category names to English for better understanding\n",
        "category_translations = {\n",
        "    'burudani': 'entertainment',\n",
        "    'uchumi': 'economy',\n",
        "    'kimataifa': 'international',\n",
        "    'kitaifa': 'national',\n",
        "    'afya': 'health',\n",
        "    'michezo': 'sports'\n",
        "}\n",
        "\n",
        "# Prepare data for visualization\n",
        "labels = df_train['label_name'].tolist()\n",
        "english_labels = [category_translations[label] for label in labels]\n",
        "\n",
        "# Create color map for categories\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
        "english_categories = list(category_translations.values())\n",
        "color_map = {eng: colors[i] for i, eng in enumerate(english_categories)}\n",
        "point_colors = [color_map[label] for label in english_labels]\n",
        "\n",
        "# 1. Visualize Autoencoder Latent Space\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "for i, eng_label in enumerate(english_categories):\n",
        "    mask = np.array(english_labels) == eng_label\n",
        "    plt.scatter(latent_representations[mask, 0], latent_representations[mask, 1],\n",
        "               c=colors[i], label=eng_label, alpha=0.6, s=10)\n",
        "plt.xlabel('Latent Dimension 1')\n",
        "plt.ylabel('Latent Dimension 2')\n",
        "plt.title('Autoencoder 2D Latent Space\\n(384D ‚Üí 2D Compression)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Apply UMAP to original embeddings for comparison\n",
        "print(\"Applying UMAP...\")\n",
        "reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "umap_embeddings = reducer.fit_transform(sentence_embeddings)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "for i, eng_label in enumerate(english_categories):\n",
        "    mask = np.array(english_labels) == eng_label\n",
        "    plt.scatter(umap_embeddings[mask, 0], umap_embeddings[mask, 1],\n",
        "               c=colors[i], label=eng_label, alpha=0.6, s=10)\n",
        "plt.xlabel('UMAP Dimension 1')\n",
        "plt.ylabel('UMAP Dimension 2')\n",
        "plt.title('UMAP Visualization of Original Embeddings')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Apply t-SNE for comparison\n",
        "print(\"Applying t-SNE...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=50)\n",
        "tsne_embeddings = tsne.fit_transform(sentence_embeddings[:5000])  # Sample for speed\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sample_english_labels = english_labels[:5000]\n",
        "for i, eng_label in enumerate(english_categories):\n",
        "    mask = np.array(sample_english_labels) == eng_label\n",
        "    plt.scatter(tsne_embeddings[mask, 0], tsne_embeddings[mask, 1],\n",
        "               c=colors[i], label=eng_label, alpha=0.6, s=10)\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.title('t-SNE Visualization (5k samples)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nCategory Translations:\")\n",
        "for swahili, english in category_translations.items():\n",
        "    print(f\"  {swahili} ‚Üí {english}\")\n",
        "\n",
        "print(\"\\nVisualization completed! Now we can clearly see the topic clusters.\")"
      ],
      "metadata": {
        "id": "UPeMypon1ToH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 21 (Continued): Identify and Interpret Distinct Topic Regions\n",
        "print(\"=== STEP 21 (Continued): Interpreting Topic Clusters ===\")\n",
        "\n",
        "# Analyze cluster characteristics in Autoencoder latent space\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 1. Show cluster centers and boundaries\n",
        "plt.subplot(1, 2, 1)\n",
        "for i, eng_label in enumerate(english_categories):\n",
        "    mask = np.array(english_labels) == eng_label\n",
        "    cluster_points = latent_representations[mask]\n",
        "    center = np.mean(cluster_points, axis=0)\n",
        "\n",
        "    plt.scatter(latent_representations[mask, 0], latent_representations[mask, 1],\n",
        "               c=colors[i], label=eng_label, alpha=0.6, s=10)\n",
        "\n",
        "    # Mark cluster center\n",
        "    plt.scatter(center[0], center[1], c='black', marker='X', s=200, edgecolors='white', linewidth=2)\n",
        "    plt.text(center[0], center[1], f' {eng_label}', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.xlabel('Latent Dimension 1')\n",
        "plt.ylabel('Latent Dimension 2')\n",
        "plt.title('Autoencoder Latent Space with Cluster Centers')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Analyze sample articles from different regions\n",
        "plt.subplot(1, 2, 2)\n",
        "# Find representative points from each quadrant\n",
        "quadrant_samples = []\n",
        "quadrants = [\n",
        "    (latent_representations[:, 0] > 0) & (latent_representations[:, 1] > 0),  # Q1\n",
        "    (latent_representations[:, 0] < 0) & (latent_representations[:, 1] > 0),  # Q2\n",
        "    (latent_representations[:, 0] < 0) & (latent_representations[:, 1] < 0),  # Q3\n",
        "    (latent_representations[:, 0] > 0) & (latent_representations[:, 1] < 0),  # Q4\n",
        "]\n",
        "\n",
        "quadrant_names = ['Q1 (Right-Top)', 'Q2 (Left-Top)', 'Q3 (Left-Bottom)', 'Q4 (Right-Bottom)']\n",
        "quadrant_colors = ['red', 'blue', 'green', 'orange']\n",
        "\n",
        "for i, (quadrant, name, color) in enumerate(zip(quadrants, quadrant_names, quadrant_colors)):\n",
        "    if np.any(quadrant):\n",
        "        sample_idx = np.where(quadrant)[0][0]  # First article in quadrant\n",
        "        quadrant_samples.append(sample_idx)\n",
        "        plt.scatter(latent_representations[sample_idx, 0], latent_representations[sample_idx, 1],\n",
        "                   c=color, s=100, marker='*', edgecolors='black', linewidth=2, label=name)\n",
        "\n",
        "# Plot all points with lower alpha\n",
        "plt.scatter(latent_representations[:, 0], latent_representations[:, 1],\n",
        "           c=point_colors, alpha=0.2, s=5)\n",
        "plt.xlabel('Latent Dimension 1')\n",
        "plt.ylabel('Latent Dimension 2')\n",
        "plt.title('Quadrant Analysis with Sample Points')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print interpretation of clusters\n",
        "print(\"\\n=== CLUSTER INTERPRETATION ===\")\n",
        "print(\"Based on the Autoencoder Latent Space:\")\n",
        "print(\"1. Sports (brown) - Clearly separated cluster, distinct from other topics\")\n",
        "print(\"2. Health (purple) - Forms a tight, well-defined cluster\")\n",
        "print(\"3. Economy (blue) - Spreads across the space, overlaps with national news\")\n",
        "print(\"4. National (orange) - Large cluster, mixes with economy and international\")\n",
        "print(\"5. International (green) - Distributed, overlaps with national news\")\n",
        "print(\"6. Entertainment (red) - Scattered, may cover diverse sub-topics\")\n",
        "\n",
        "print(f\"\\nCompression Effectiveness: 384D ‚Üí 2D while maintaining topic separation!\")\n",
        "print(\"Sports and Health articles show the clearest separation in latent space.\")"
      ],
      "metadata": {
        "id": "B22VZpNa3bhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 22: Exploration - Different Autoencoder Architectures\n",
        "print(\"=== STEP 22: Experiment with Different Autoencoder Configurations ===\")\n",
        "\n",
        "def create_autoencoder_variant(input_dim, encoding_dim, hidden_layers=3, activation='relu'):\n",
        "    \"\"\"Create autoencoder with different architectures\"\"\"\n",
        "    # Encoder\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "    # Build encoder with variable depth\n",
        "    encoded = input_layer\n",
        "    for i in range(hidden_layers):\n",
        "        units = 128 // (2 ** i)  # Decreasing units: 128, 64, 32, etc.\n",
        "        encoded = Dense(units, activation=activation)(encoded)\n",
        "\n",
        "    latent = Dense(encoding_dim, activation=activation)(encoded)\n",
        "\n",
        "    # Decoder (symmetric)\n",
        "    decoded = latent\n",
        "    for i in range(hidden_layers-1, -1, -1):\n",
        "        units = 128 // (2 ** i)\n",
        "        decoded = Dense(units, activation=activation)(decoded)\n",
        "\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    return autoencoder\n",
        "\n",
        "# Test different configurations\n",
        "configurations = [\n",
        "    {'hidden_layers': 2, 'activation': 'relu', 'encoding_dim': 2, 'name': 'Shallow-ReLU-2D'},\n",
        "    {'hidden_layers': 4, 'activation': 'relu', 'encoding_dim': 2, 'name': 'Deep-ReLU-2D'},\n",
        "    {'hidden_layers': 3, 'activation': 'tanh', 'encoding_dim': 2, 'name': 'Medium-Tanh-2D'},\n",
        "    {'hidden_layers': 3, 'activation': 'relu', 'encoding_dim': 3, 'name': 'Medium-ReLU-3D'},\n",
        "]\n",
        "\n",
        "# Use smaller sample for faster experimentation\n",
        "sample_size = 2000\n",
        "X_sample = sentence_embeddings[:sample_size]\n",
        "X_train_small, X_val_small = train_test_split(X_sample, test_size=0.2, random_state=42)\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"Testing different autoencoder configurations...\")\n",
        "for config in configurations:\n",
        "    print(f\"\\nTesting: {config['name']}\")\n",
        "\n",
        "    # Create and train autoencoder\n",
        "    autoencoder = create_autoencoder_variant(\n",
        "        input_dim=X_sample.shape[1],\n",
        "        encoding_dim=config['encoding_dim'],\n",
        "        hidden_layers=config['hidden_layers'],\n",
        "        activation=config['activation']\n",
        "    )\n",
        "\n",
        "    history = autoencoder.fit(\n",
        "        X_train_small, X_train_small,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val_small, X_val_small),\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    final_loss = history.history['val_loss'][-1]\n",
        "    config['final_loss'] = final_loss\n",
        "    results.append(config)\n",
        "\n",
        "    print(f\"  Final validation loss: {final_loss:.6f}\")\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n=== COMPARISON RESULTS ===\")\n",
        "print(\"Configuration\\t\\t\\tFinal Val Loss\")\n",
        "print(\"-\" * 50)\n",
        "for result in sorted(results, key=lambda x: x['final_loss']):\n",
        "    print(f\"{result['name']:25} {result['final_loss']:.6f}\")\n",
        "\n",
        "# Visualize comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "config_names = [r['name'] for r in results]\n",
        "losses = [r['final_loss'] for r in results]\n",
        "\n",
        "plt.bar(config_names, losses, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
        "plt.xlabel('Autoencoder Configuration')\n",
        "plt.ylabel('Validation Loss (MSE)')\n",
        "plt.title('Comparison of Different Autoencoder Architectures')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nBest performing configuration:\", min(results, key=lambda x: x['final_loss'])['name'])"
      ],
      "metadata": {
        "id": "yj59R5yf4JpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 22 Detailed Explanation\n",
        "print(\"=== DETAILED EXPLANATION OF AUTOENCODER EXPERIMENTS ===\")\n",
        "\n",
        "print(\"\\n1. ENCODER DEPTHS (Number of Hidden Layers):\")\n",
        "print(\"   - Shallow-ReLU-2D: 2 hidden layers (128 ‚Üí 64 ‚Üí 2)\")\n",
        "print(\"   - Medium-ReLU-3D: 3 hidden layers (128 ‚Üí 64 ‚Üí 32 ‚Üí 3)\")\n",
        "print(\"   - Deep-ReLU-2D: 4 hidden layers (128 ‚Üí 64 ‚Üí 32 ‚Üí 16 ‚Üí 2)\")\n",
        "\n",
        "print(\"\\n2. ACTIVATION FUNCTIONS:\")\n",
        "print(\"   - ReLU (Rectified Linear Unit): f(x) = max(0, x)\")\n",
        "print(\"     ‚Üí Used in: Shallow-ReLU-2D, Deep-ReLU-2D, Medium-ReLU-3D\")\n",
        "print(\"     ‚Üí Pros: Prevents vanishing gradient, computationally efficient\")\n",
        "print(\"     ‚Üí Cons: Can cause 'dying ReLU' problem\")\n",
        "print(\"   - Tanh (Hyperbolic Tangent): f(x) = (e^x - e^-x)/(e^x + e^-x)\")\n",
        "print(\"     ‚Üí Used in: Medium-Tanh-2D\")\n",
        "print(\"     ‚Üí Pros: Output range [-1, 1], zero-centered\")\n",
        "print(\"     ‚Üí Cons: Can suffer from vanishing gradients\")\n",
        "\n",
        "print(\"\\n3. LATENT DIMENSIONS:\")\n",
        "print(\"   - 2D Latent Space: Shallow-ReLU-2D, Deep-ReLU-2D, Medium-Tanh-2D\")\n",
        "print(\"     ‚Üí High compression (384:1 ratio)\")\n",
        "print(\"     ‚Üí Good for 2D visualization\")\n",
        "print(\"   - 3D Latent Space: Medium-ReLU-3D\")\n",
        "print(\"     ‚Üí Less compression (128:1 ratio)\")\n",
        "print(\"     ‚Üí Could capture more information\")\n",
        "\n",
        "print(\"\\n4. ARCHITECTURE DETAILS:\")\n",
        "print(\"   All autoencoders had symmetric encoder-decoder structure\")\n",
        "print(\"   Hidden layer units followed geometric progression: 128, 64, 32, 16\")\n",
        "print(\"   All used Adam optimizer with MSE loss function\")\n",
        "print(\"   Trained for 20 epochs on 2000 samples\")\n",
        "\n",
        "print(\"\\n5. RESULTS ANALYSIS:\")\n",
        "print(\"   - BEST: Shallow-ReLU-2D (val_loss: 0.021384)\")\n",
        "print(\"     ‚Üí Suggests simple architectures work well for this task\")\n",
        "print(\"     ‚Üí 2 layers sufficient to capture essential patterns\")\n",
        "print(\"   - WORST: Medium-Tanh-2D (val_loss: 0.022472)\")\n",
        "print(\"     ‚Üí Tanh underperformed ReLU for this text embedding task\")\n",
        "print(\"   - 3D vs 2D: Medium-ReLU-3D (0.022074) worse than 2D variants\")\n",
        "print(\"     ‚Üí Extra dimension didn't improve reconstruction\")\n",
        "print(\"     ‚Üí Suggests 2D is sufficient for topic separation\")\n",
        "\n",
        "print(\"\\n6. KEY INSIGHTS:\")\n",
        "print(\"   - Simpler architectures often perform better for text embedding compression\")\n",
        "print(\"   - ReLU activation works better than Tanh for this specific task\")\n",
        "print(\"   - 2D latent space provides sufficient dimensionality for topic separation\")\n",
        "print(\"   - Overly deep networks may overfit or learn unnecessary complexity\")\n",
        "print(\"   - The optimal balance: 2 hidden layers + ReLU + 2D latent space\")"
      ],
      "metadata": {
        "id": "L8w515T25qOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 23: Train Neural Topic Model (NTM)\n",
        "print(\"=== STEP 23: Neural Topic Model (NTM) Training ===\")\n",
        "\n",
        "# First, let's prepare the text data for NTM\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "import gensim\n",
        "\n",
        "print(\"Preparing text data for Neural Topic Modeling...\")\n",
        "\n",
        "# More comprehensive text preprocessing for topic modeling\n",
        "def preprocess_for_topic_modeling(text):\n",
        "    \"\"\"Preprocess text for topic modeling\"\"\"\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and short words\n",
        "    tokens = [word for word in tokens if word not in swahili_stopwords and len(word) > 2]\n",
        "    return tokens\n",
        "\n",
        "# Process a sample of texts for topic modeling\n",
        "sample_size = 5000  # Use smaller sample for computational efficiency\n",
        "sample_texts = df_train['text'].head(sample_size).tolist()\n",
        "\n",
        "print(\"Tokenizing texts...\")\n",
        "tokenized_texts = [preprocess_for_topic_modeling(text) for text in sample_texts]\n",
        "\n",
        "# Create dictionary and corpus\n",
        "print(\"Creating dictionary and corpus...\")\n",
        "dictionary = corpora.Dictionary(tokenized_texts)\n",
        "dictionary.filter_extremes(no_below=10, no_above=0.5)  # Filter rare and common words\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
        "\n",
        "print(f\"Dictionary size: {len(dictionary)}\")\n",
        "print(f\"Corpus size: {len(corpus)}\")\n",
        "\n",
        "# Train LDA model (Neural Topic Model)\n",
        "print(\"\\nTraining LDA (Neural Topic Model)...\")\n",
        "num_topics = 6  # Same as our original categories\n",
        "\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=10,\n",
        "    alpha='auto',\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "print(\"LDA training completed!\")\n",
        "\n",
        "# Display the topics\n",
        "print(\"\\n=== LDA TOPICS ===\")\n",
        "topics = lda_model.print_topics(num_words=10)\n",
        "for idx, topic in topics:\n",
        "    print(f\"Topic {idx}: {topic}\")"
      ],
      "metadata": {
        "id": "mO7cl1lB6PMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 23 (Continued): Translate Topics and Compare with Autoencoder\n",
        "print(\"=== STEP 23 (Continued): Topic Translation and Comparison ===\")\n",
        "\n",
        "# Translate the key topic words to understand the topics\n",
        "print(\"\\n=== TRANSLATED LDA TOPICS ===\")\n",
        "topic_translations = {}\n",
        "\n",
        "for idx, topic in topics:\n",
        "    # Extract the top words from the topic string\n",
        "    topic_words = re.findall(r'\\*\"([^\"]+)\"', topic)\n",
        "\n",
        "    print(f\"\\nTopic {idx}:\")\n",
        "    translated_words = []\n",
        "    for word in topic_words[:5]:  # Translate top 5 words\n",
        "        try:\n",
        "            translation = translator.translate(word, src='sw', dest='en')\n",
        "            translated_words.append((word, translation.text))\n",
        "            print(f\"  {word} ‚Üí {translation.text}\")\n",
        "        except:\n",
        "            translated_words.append((word, \"translation_error\"))\n",
        "            print(f\"  {word} ‚Üí translation_error\")\n",
        "\n",
        "    topic_translations[idx] = translated_words\n",
        "\n",
        "# Map LDA topics to original categories based on content\n",
        "print(\"\\n=== TOPIC INTERPRETATION ===\")\n",
        "topic_interpretations = {\n",
        "    0: \"Government & Leadership (serikali‚Üígovernment, mkuu‚Üíchief, rais‚Üípresident)\",\n",
        "    1: \"National Economy & Development (nchi‚Üícountry, fedha‚Üímoney, ujenzi‚Üíconstruction)\",\n",
        "    2: \"Justice & Public Services (mahakama‚Üícourt, huduma‚Üíservices, kesi‚Üícase)\",\n",
        "    3: \"Sports (timu‚Üíteam, mchezo‚Üígame, yanga‚ÜíYanga [team], simba‚ÜíSimba [team])\",\n",
        "    4: \"International Relations (rais‚Üípresident, marekani‚ÜíAmerica, nchini‚Üíin the country)\",\n",
        "    5: \"Health & Social Issues (watu‚Üípeople, watoto‚Üíchildren, afya‚Üíhealth)\"\n",
        "}\n",
        "\n",
        "for topic_idx, interpretation in topic_interpretations.items():\n",
        "    print(f\"Topic {topic_idx}: {interpretation}\")\n",
        "\n",
        "# Compare with original categories\n",
        "print(\"\\n=== COMPARISON WITH ORIGINAL CATEGORIES ===\")\n",
        "print(\"Original Categories vs LDA Discovered Topics:\")\n",
        "original_categories = {\n",
        "    'burudani': 'entertainment',\n",
        "    'uchumi': 'economy',\n",
        "    'kimataifa': 'international',\n",
        "    'kitaifa': 'national',\n",
        "    'afya': 'health',\n",
        "    'michezo': 'sports'\n",
        "}\n",
        "\n",
        "print(\"\\nOriginal Categories:\")\n",
        "for sw, eng in original_categories.items():\n",
        "    print(f\"  {sw} ‚Üí {eng}\")\n",
        "\n",
        "print(\"\\nLDA Discovered Topics:\")\n",
        "for idx in range(num_topics):\n",
        "    print(f\"  Topic {idx} ‚Üí {topic_interpretations[idx].split('(')[0].strip()}\")\n",
        "\n",
        "print(\"\\n=== KEY INSIGHTS ===\")\n",
        "print(\"1. LDA successfully identified distinct thematic areas\")\n",
        "print(\"2. Sports (Topic 3) is clearly separated in both methods\")\n",
        "print(\"3. Government topics (0,1,4) show some overlap in LDA\")\n",
        "print(\"4. Health (Topic 5) appears mixed with social issues\")\n",
        "print(\"5. Autoencoder provided cleaner separation of original categories\")\n",
        "print(\"6. LDA gives interpretable word distributions per topic\")"
      ],
      "metadata": {
        "id": "zshA4-D97OOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 23 (Final): Comprehensive Comparison between Autoencoder and NTM\n",
        "print(\"=== STEP 23 (Final): Autoencoder vs Neural Topic Model Comparison ===\")\n",
        "\n",
        "# Create comparison visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Autoencoder Latent Space (from Step 21)\n",
        "ax1 = axes[0, 0]\n",
        "for i, eng_label in enumerate(english_categories):\n",
        "    mask = np.array(english_labels) == eng_label\n",
        "    ax1.scatter(latent_representations[mask, 0], latent_representations[mask, 1],\n",
        "               c=colors[i], label=eng_label, alpha=0.6, s=10)\n",
        "ax1.set_xlabel('Latent Dimension 1')\n",
        "ax1.set_ylabel('Latent Dimension 2')\n",
        "ax1.set_title('Autoencoder: 2D Latent Space\\n(Preserves Original Categories)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. UMAP of original embeddings for reference\n",
        "ax2 = axes[0, 1]\n",
        "for i, eng_label in enumerate(english_categories):\n",
        "    mask = np.array(english_labels) == eng_label\n",
        "    ax2.scatter(umap_embeddings[mask, 0], umap_embeddings[mask, 1],\n",
        "               c=colors[i], label=eng_label, alpha=0.6, s=10)\n",
        "ax2.set_xlabel('UMAP Dimension 1')\n",
        "ax2.set_ylabel('UMAP Dimension 2')\n",
        "ax2.set_title('UMAP: Original Embedding Structure')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. LDA Topic Distribution (Visualize topic proportions)\n",
        "ax3 = axes[1, 0]\n",
        "# Get topic distributions for sample documents\n",
        "topic_distributions = []\n",
        "for doc in corpus[:1000]:  # Sample for visualization\n",
        "    topic_dist = lda_model.get_document_topics(doc, minimum_probability=0)\n",
        "    topic_probs = [prob for _, prob in topic_dist]\n",
        "    topic_distributions.append(topic_probs)\n",
        "\n",
        "topic_distributions = np.array(topic_distributions)\n",
        "\n",
        "# Plot topic distribution heatmap\n",
        "im = ax3.imshow(topic_distributions.T, aspect='auto', cmap='viridis')\n",
        "ax3.set_xlabel('Documents')\n",
        "ax3.set_ylabel('Topics')\n",
        "ax3.set_title('LDA: Topic Distributions across Documents')\n",
        "ax3.set_yticks(range(num_topics))\n",
        "ax3.set_yticklabels([f'Topic {i}' for i in range(num_topics)])\n",
        "plt.colorbar(im, ax=ax3, label='Topic Probability')\n",
        "\n",
        "# 4. Comparison Summary\n",
        "ax4 = axes[1, 1]\n",
        "ax4.axis('off')\n",
        "\n",
        "comparison_text = [\n",
        "    \"AUTOENCODER VS NEURAL TOPIC MODEL COMPARISON\",\n",
        "    \"\",\n",
        "    \"üü¢ AUTOENCODER STRENGTHS:\",\n",
        "    \"‚Ä¢ Preserves original category structure\",\n",
        "    \"‚Ä¢ Clear visual cluster separation\",\n",
        "    \"‚Ä¢ Continuous latent representations\",\n",
        "    \"‚Ä¢ Good for dimensionality reduction\",\n",
        "    \"\",\n",
        "    \"üîµ NTM (LDA) STRENGTHS:\",\n",
        "    \"‚Ä¢ Interpretable topic-word distributions\",\n",
        "    \"‚Ä¢ Probabilistic topic assignments\",\n",
        "    \"‚Ä¢ Discovered meaningful themes\",\n",
        "    \"‚Ä¢ Handles overlapping topics\",\n",
        "    \"\",\n",
        "    \"üü° KEY DIFFERENCES:\",\n",
        "    \"‚Ä¢ Autoencoder: 2D continuous space\",\n",
        "    \"‚Ä¢ NTM: 6 discrete probabilistic topics\",\n",
        "    \"‚Ä¢ Sports clearly separated in both\",\n",
        "    \"‚Ä¢ Government topics more nuanced in NTM\",\n",
        "    \"\",\n",
        "    \"üéØ USE CASES:\",\n",
        "    \"‚Ä¢ Autoencoder: Visualization & compression\",\n",
        "    \"‚Ä¢ NTM: Topic interpretation & analysis\"\n",
        "]\n",
        "\n",
        "for i, line in enumerate(comparison_text):\n",
        "    ax4.text(0.02, 0.98 - i*0.04, line, transform=ax4.transAxes,\n",
        "             fontsize=10, verticalalignment='top')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== FINAL COMPARISON SUMMARY ===\")\n",
        "print(\"Autoencoder successfully compressed 384D ‚Üí 2D while maintaining category structure\")\n",
        "print(\"NTM discovered 6 interpretable topics that align with but refine the original categories\")\n",
        "print(\"Both methods show Sports as the most distinct category\")\n",
        "print(\"Autoencoder better for visualization, NTM better for topic interpretation\")"
      ],
      "metadata": {
        "id": "xm8DHyo_76Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 24: Multilingual Semantic Cluster Analysis\n",
        "print(\"=== STEP 24: Multilingual Semantic Cluster Analysis ===\")\n",
        "\n",
        "# Step 1: Translate a sample of Swahili articles to English\n",
        "print(\"Step 1: Translating Swahili articles to English...\")\n",
        "\n",
        "# Use a smaller sample for translation (300 articles for speed)\n",
        "sample_size = 300\n",
        "swahili_sample = df_train['text'].head(sample_size).apply(preprocess_text).tolist()\n",
        "swahili_labels = df_train['label_name'].head(sample_size).tolist()\n",
        "\n",
        "print(f\"Translating {len(swahili_sample)} Swahili articles to English...\")\n",
        "\n",
        "english_sample = []\n",
        "for i, text in enumerate(swahili_sample):\n",
        "    if i % 50 == 0:\n",
        "        print(f\"Translated {i}/{sample_size} articles...\")\n",
        "    try:\n",
        "        # Translate first 800 characters to avoid API limits\n",
        "        translated = translator.translate(text[:800], src='sw', dest='en')\n",
        "        english_sample.append(translated.text)\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error for article {i}: {e}\")\n",
        "        english_sample.append(\"Translation unavailable\")\n",
        "\n",
        "print(\"Translation completed!\")\n",
        "print(f\"Sample Swahili: {swahili_sample[0][:100]}...\")\n",
        "print(f\"Sample English: {english_sample[0][:100]}...\")"
      ],
      "metadata": {
        "id": "xd-qUXHlDwQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 24 (Continued): Create embeddings and visualize multilingual clusters\n",
        "print(\"\\nStep 2: Creating embeddings for both languages...\")\n",
        "\n",
        "print(\"Generating Swahili embeddings...\")\n",
        "swahili_embeddings = model.encode(swahili_sample, show_progress_bar=True)\n",
        "\n",
        "print(\"Generating English embeddings...\")\n",
        "# Filter out any failed translations\n",
        "valid_english_indices = [i for i, text in enumerate(english_sample) if text != \"Translation unavailable\"]\n",
        "valid_english_texts = [english_sample[i] for i in valid_english_indices]\n",
        "valid_swahili_indices = valid_english_indices  # Keep corresponding Swahili articles\n",
        "\n",
        "english_embeddings = model.encode(valid_english_texts, show_progress_bar=True)\n",
        "\n",
        "print(f\"Swahili embeddings shape: {swahili_embeddings.shape}\")\n",
        "print(f\"English embeddings shape: {english_embeddings.shape}\")\n",
        "print(f\"Valid translations: {len(valid_english_texts)}/{sample_size}\")\n",
        "\n",
        "# Step 3: Apply UMAP to both language embeddings\n",
        "print(\"\\nStep 3: Applying UMAP to compare semantic spaces...\")\n",
        "\n",
        "# Combine both language embeddings for consistent UMAP\n",
        "combined_embeddings = np.vstack([swahili_embeddings[valid_swahili_indices], english_embeddings])\n",
        "combined_reducer = umap.UMAP(n_components=2, random_state=42, metric='cosine')\n",
        "combined_umap = combined_reducer.fit_transform(combined_embeddings)\n",
        "\n",
        "# Split back into Swahili and English\n",
        "swahili_umap = combined_umap[:len(valid_swahili_indices)]\n",
        "english_umap = combined_umap[len(valid_swahili_indices):]\n",
        "\n",
        "print(\"UMAP transformation completed!\")\n",
        "\n",
        "# Get corresponding labels for valid samples\n",
        "valid_labels = [swahili_labels[i] for i in valid_swahili_indices]\n",
        "valid_english_labels = [category_translations[label] for label in valid_labels]\n",
        "\n",
        "print(f\"\\nReady to visualize {len(valid_swahili_indices)} article pairs!\")"
      ],
      "metadata": {
        "id": "ENpVvB6VFPgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 24 (Final): Visualize Language-Invariant Semantic Clusters\n",
        "print(\"=== STEP 24 (Final): Multilingual Cluster Visualization ===\")\n",
        "\n",
        "# Create the visualization with different markers for each language\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Define markers and colors\n",
        "markers = {'Swahili': 'o', 'English': '^'}  # Circles for Swahili, Triangles for English\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
        "english_categories_list = list(category_translations.values())\n",
        "\n",
        "# Plot Swahili articles (circles)\n",
        "for i, eng_label in enumerate(english_categories_list):\n",
        "    # Find indices for this category in valid samples\n",
        "    mask = np.array(valid_english_labels) == eng_label\n",
        "    if np.any(mask):\n",
        "        plt.scatter(swahili_umap[mask, 0], swahili_umap[mask, 1],\n",
        "                   c=colors[i], marker='o', label=f'Swahili {eng_label}',\n",
        "                   alpha=0.7, s=60, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "# Plot English articles (triangles)\n",
        "for i, eng_label in enumerate(english_categories_list):\n",
        "    mask = np.array(valid_english_labels) == eng_label\n",
        "    if np.any(mask):\n",
        "        plt.scatter(english_umap[mask, 0], english_umap[mask, 1],\n",
        "                   c=colors[i], marker='^', label=f'English {eng_label}',\n",
        "                   alpha=0.7, s=60, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "plt.xlabel('UMAP Dimension 1')\n",
        "plt.ylabel('UMAP Dimension 2')\n",
        "plt.title('Language-Invariant Semantic Clusters\\n‚óè = Swahili   ‚ñ≤ = English\\n(Same Color = Same Topic)')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate distance between each Swahili article and its English translation\n",
        "print(\"\\n=== ANALYSIS: Distance Between Translations ===\")\n",
        "distances = []\n",
        "for i in range(len(swahili_umap)):\n",
        "    dist = np.linalg.norm(swahili_umap[i] - english_umap[i])\n",
        "    distances.append(dist)\n",
        "\n",
        "avg_distance = np.mean(distances)\n",
        "print(f\"Average distance between Swahili articles and their English translations: {avg_distance:.4f}\")\n",
        "\n",
        "# Check if same-topic articles cluster together regardless of language\n",
        "print(\"\\n=== CLUSTER ANALYSIS ===\")\n",
        "print(\"If clusters are LANGUAGE-INVARIANT, we should see:\")\n",
        "print(\"‚Ä¢ Brown circles (Swahili Sports) mixed with brown triangles (English Sports)\")\n",
        "print(\"‚Ä¢ Purple circles (Swahili Health) mixed with purple triangles (English Health)\")\n",
        "print(\"‚Ä¢ etc.\")\n",
        "\n",
        "print(\"\\nIf clusters are LANGUAGE-DEPENDENT, we would see:\")\n",
        "print(\"‚Ä¢ All circles (Swahili) on one side\")\n",
        "print(\"‚Ä¢ All triangles (English) on the other side\")\n",
        "\n",
        "print(f\"\\nAverage translation pair distance: {avg_distance:.4f}\")\n",
        "print(\"Lower distance = Better language invariance\")\n",
        "print(\"Higher distance = Language-dependent clustering\")\n",
        "\n",
        "# Show some example distances by category\n",
        "print(\"\\n=== DISTANCE BY TOPIC ===\")\n",
        "for i, eng_label in enumerate(english_categories_list):\n",
        "    mask = np.array(valid_english_labels) == eng_label\n",
        "    if np.any(mask):\n",
        "        topic_distances = [distances[j] for j in range(len(distances)) if mask[j]]\n",
        "        avg_topic_dist = np.mean(topic_distances)\n",
        "        print(f\"{eng_label:15}: {avg_topic_dist:.4f} (lower = better cross-language understanding)\")"
      ],
      "metadata": {
        "id": "RBlXlm1tFj3y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
